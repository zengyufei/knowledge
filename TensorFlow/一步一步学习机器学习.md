环境：
    Anaconda 5.0
    python 3.6
    tensorflow CPU
    CPU: I5 6500 3.20G
    内存： 8G

```python
import tensorflow as tf 
hello = tf.constant('Hello, TensorFlow!') 
sess = tf.Session()
print(sess.run(hello))
```

    b'Hello, TensorFlow!'
    


```python
import tensorflow as tf 
import numpy as np  
# 使用 NumPy 生成假数据(phony data), 总共 100 个点.  
x_data = np.float32(np.random.rand(2, 100)) # 随机输入  
y_data = np.dot([0.100, 0.200], x_data) + 0.300  
# 构造一个线性模型  
# 用Variable来定义变量  
b = tf.Variable(tf.zeros([1]))  
W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0))  
y = tf.matmul(W, x_data) + b  
# 最小化方差  
loss = tf.reduce_mean(tf.square(y - y_data))  
optimizer = tf.train.GradientDescentOptimizer(0.5)  
train = optimizer.minimize(loss)  
# 初始化变量  
init = tf.global_variables_initializer()  
# 启动图 (graph)  
sess = tf.Session()  
sess.run(init)  
# 拟合平面  
for step in range(0, 201):  
    sess.run(train)  
    if step % 20 == 0:  
        print(step, sess.run(W), sess.run(b))  
```

    0 [[ 0.00957194  0.73165143]] [ 0.08583613]
    20 [[ 0.09841096  0.32746667]] [ 0.22735308]
    40 [[ 0.10620412  0.23260804]] [ 0.2778078]
    60 [[ 0.10338217  0.20865324]] [ 0.29316357]
    80 [[ 0.10136884  0.20237812]] [ 0.29788142]
    100 [[ 0.1004955   0.20067427]] [ 0.29934072]
    120 [[ 0.10016964  0.20019624]] [ 0.29979426]
    140 [[ 0.10005627  0.2000583 ]] [ 0.29993567]
    160 [[ 0.10001832  0.2000176 ]] [ 0.29997984]
    180 [[ 0.1000059   0.20000537]] [ 0.29999366]
    200 [[ 0.10000189  0.20000166]] [ 0.29999802]
    
将下载好的 mnist 数据包 *.gz 格式放入本文件所在目录下 Mnist_data 文件夹，并测试。
mnist（国家标注技术研究所的数据集） 是什么：http://blog.csdn.net/vagrantabc2017/article/details/77063792

```python
from tensorflow.examples.tutorials.mnist import input_data
MNIST_data_folder = './Mnist_data'
mnist = input_data.read_data_sets(MNIST_data_folder,one_hot=True)
print(mnist.train.next_batch(1))
print('end')
```

    Extracting ./Mnist_data\train-images-idx3-ubyte.gz
    Extracting ./Mnist_data\train-labels-idx1-ubyte.gz
    Extracting ./Mnist_data\t10k-images-idx3-ubyte.gz
    Extracting ./Mnist_data\t10k-labels-idx1-ubyte.gz
    (array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.23137257,  1.        ,  0.99607849,  0.99607849,
             0.58823532,  0.38823533,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.04313726,  0.84705889,
             0.99607849,  0.99215692,  0.99215692,  0.99215692,  0.99215692,
             0.69803923,  0.56862748,  0.28627452,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.07450981,  0.99215692,  0.99607849,  0.99215692,
             0.86666673,  0.8588236 ,  0.99215692,  0.99607849,  0.99215692,
             0.92156869,  0.57254905,  0.32156864,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.04313726,
             0.8705883 ,  0.99607849,  0.99215692,  0.16470589,  0.01568628,
             0.21176472,  0.44313729,  0.89411771,  0.95686281,  0.99215692,
             0.91372555,  0.18039216,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.45882356,  0.99607849,
             0.99215692,  0.49803925,  0.        ,  0.        ,  0.        ,
             0.        ,  0.14117648,  0.71372551,  0.99215692,  0.70980394,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.36078432,  1.        ,  0.99607849,  0.49803925,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.43137258,  0.99607849,  0.71372551,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.02745098,
             0.99607849,  0.99215692,  0.49803925,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.42745101,  0.99215692,
             0.70980394,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.99607849,  0.99215692,
             0.54509807,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.42745101,  0.99215692,  0.81176478,  0.02745098,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.84313732,  0.99215692,  0.84705889,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.45490199,
             0.99215692,  0.70980394,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.28627452,
             0.96862751,  0.47058827,  0.        ,  0.        ,  0.        ,
             0.        ,  0.05490196,  0.85490203,  0.99215692,  0.70980394,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.37254903,
             0.99607849,  0.99607849,  0.61176473,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.57254905,  0.99215692,  0.91372555,
             0.15294118,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.03137255,
             0.8705883 ,  0.99215692,  0.63529414,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.49803925,  0.99215692,  0.98431379,
             0.4666667 ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.69411767,  0.99215692,  0.79215693,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.28627452,  0.96862751,  0.99607849,
             0.4666667 ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.63921571,  0.99215692,  0.96078438,  0.16862746,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.63921571,  0.99215692,
             0.92156869,  0.21960786,  0.04313726,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.61176473,  0.99215692,  0.99215692,  0.99215692,
             0.41176474,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.08235294,
             0.81568635,  0.94117653,  0.60784316,  0.08235294,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ,  0.        ,  0.        ,  0.        ]], dtype=float32), array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]]))
    end
    
下面这段代码应该能直接运行，但是参数和官网有所区别（显卡太渣了，按照官网的参数跑会报OOM错误），最后的测试准确率也就0.9683（只迭代了2000次，调成20000次后会更接近于官网的结果）。不过这段代码涉及到了很多Tensorflow的基本知识，比如会话、占位符、图、构建图、Feed等等。

```python
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import gzip
import os
import tempfile

import numpy
from six.moves import urllib
from six.moves import xrange

import tensorflow as tf
from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets


class Mnist():

    # 第一层卷积输出特征数量
    out_features1 = 12
    # 第二层卷积输出特征数量
    out_features2 = 24
    # 全连接层神经元数量
    con_neurons = 512

    def __init__(self, path):
        self.sess = tf.Session()
        self.data = read_data_sets(path, one_hot=True)

    """权重初始化函数"""
    def weight_variable(self, shape):
        # 输出的随机数满足 截尾正态分布
        initial = tf.truncated_normal(shape, stddev=0.1)
        return tf.Variable(initial)

    """偏置初始化函数"""
    def bias_variable(self, shape):
        initial = tf.constant(0.1, shape=shape)
        return tf.Variable(initial)

    """二维卷积函数"""
    def conv2d(self, x, W):
        # input : 输入数据[batch, in_height, in_width, in_channels]
        # filter : 卷积窗口[filter_height, filter_width, in_channels, out_channels]
        # strides: 卷积核每次移动步数，对应着输入的维度方向
        # padding='SAME' ： 输入和输出的张量形状相同
        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')

    """池化函数"""
    def max_pool_2x2(self, x):
        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

    """构建卷积层"""
    def create_conv_layer(self, input, input_features, out_features):
        W_conv = self.weight_variable([5, 5, input_features, out_features])
        b_conv = self.bias_variable([out_features])
        h_conv = tf.nn.relu(self.conv2d(input, W_conv) + b_conv)
        h_pool = self.max_pool_2x2(h_conv)
        return h_pool

    """构建密集连接层"""
    def create_con_layer(self, h_pool_flat, input_freatures, con_neurons):
        W_fc = self.weight_variable([input_freatures, con_neurons])
        b_fc = self.bias_variable([con_neurons])
        h_fc1 = tf.nn.relu(tf.matmul(h_pool_flat, W_fc) + b_fc)
        return h_fc1

    """神经网络构建"""
    def build(self):
        # 输入
        self.x = tf.placeholder(tf.float32, shape=[None, 784])
        x_image = tf.reshape(self.x, [-1, 28, 28, 1])
        # 输出
        self.y_ = tf.placeholder(tf.float32, shape=[None, 10])

        # 第一层
        h_pool1 = self.create_conv_layer(x_image, 1, self.out_features1)
        # 第二层
        h_pool2 = self.create_conv_layer(h_pool1, self.out_features1, self.out_features2)

        # 密集连接层
        h_pool2_flat_freatures = 7*7*self.out_features2
        h_pool2_flat = tf.reshape(h_pool2, [-1, h_pool2_flat_freatures])
        h_fc = self.create_con_layer(h_pool2_flat, h_pool2_flat_freatures, self.con_neurons)

        # Dropout
        self.keep_prob = tf.placeholder("float")
        h_fc1_drop = tf.nn.dropout(h_fc, self.keep_prob)

        # 输出层
        W_fc = self.weight_variable([self.con_neurons, 10])
        b_fc = self.bias_variable([10])
        y_conv = tf.matmul(h_fc1_drop, W_fc) + b_fc

        # 评价
        correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(self.y_, 1))
        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

        # 训练方法
        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_, logits=y_conv))
        self.train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)

    """训练"""
    def train(self):
        self.sess.run(tf.global_variables_initializer())
        for i in range(2000):
            batch = self.data.train.next_batch(50)
            if i % 100 == 0:
                print('Training step %d:' % i)
                self.eval(batch[0], batch[1], 1.0)
            self.sess.run(self.train_step, {self.x: batch[0], self.y_: batch[1], self.keep_prob: 0.5})

    """评价"""
    def eval(self, x, y, keep_prob):
        train_accuracy = self.sess.run(self.accuracy, {self.x: x, self.y_: y, self.keep_prob: keep_prob})
        print('     accuracy %g' % train_accuracy)
        return train_accuracy

    """关闭会话"""
    def close(self):
        self.sess.close()


""" START """
mnist = Mnist('MNIST_data/')
mnist.build()
mnist.train()
print('\n----- Test -----')
mnist.eval(mnist.data.test.images, mnist.data.test.labels, 1.0)
mnist.close()
```

    Extracting MNIST_data/train-images-idx3-ubyte.gz
    Extracting MNIST_data/train-labels-idx1-ubyte.gz
    Extracting MNIST_data/t10k-images-idx3-ubyte.gz
    Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
    Training step 0:
         accuracy 0.14
    Training step 100:
         accuracy 0.62
    Training step 200:
         accuracy 0.94
    Training step 300:
         accuracy 0.92
    Training step 400:
         accuracy 0.86
    Training step 500:
         accuracy 0.9
    Training step 600:
         accuracy 0.92
    Training step 700:
         accuracy 0.96
    Training step 800:
         accuracy 0.96
    Training step 900:
         accuracy 0.92
    Training step 1000:
         accuracy 0.96
    Training step 1100:
         accuracy 1
    Training step 1200:
         accuracy 0.9
    Training step 1300:
         accuracy 1
    Training step 1400:
         accuracy 0.96
    Training step 1500:
         accuracy 0.94
    Training step 1600:
         accuracy 0.98
    Training step 1700:
         accuracy 0.92
    Training step 1800:
         accuracy 0.96
    Training step 1900:
         accuracy 1
    
    ----- Test -----
         accuracy 0.9678
    


```python

```
